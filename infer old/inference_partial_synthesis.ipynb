{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import commons\n",
    "import utils\n",
    "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
    "from models import SynthesizerTrn\n",
    "from text_JP.symbols import symbols as jpn_symbols # text_JPのsymbolsを使用\n",
    "from text_JP.phonemize import Phonemizer # Phonemizerをインポート\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Phonemizerの初期化とシンボルIDへのマッピング\n",
    "_phonemizer = Phonemizer()\n",
    "_symbol_to_id = {s: i for i, s in enumerate(jpn_symbols)}\n",
    "\n",
    "def _symbols_to_sequence(symbols_list):\n",
    "    sequence = []\n",
    "    for s in symbols_list:\n",
    "        if s in _symbol_to_id:\n",
    "            sequence.append(_symbol_to_id[s])\n",
    "    return sequence\n",
    "\n",
    "def get_text(text, hps):\n",
    "    # Phonemizerを使用してテキストを音素列に変換\n",
    "    phonemes_str = _phonemizer(text) # 例: \"k o N n i t i h a\"\n",
    "    phonemes_list = phonemes_str.split(' ') # 例: [\"k\", \"o\", \"N\", ...]\n",
    "\n",
    "    # 音素列をIDのシーケンスに変換\n",
    "    text_norm = _symbols_to_sequence(phonemes_list)\n",
    "    \n",
    "    if hps.data.add_blank:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Synthesis from Latent Representation (UUDB Phonemizer Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Model Loading\n",
    "# UUDBモデルの設定ファイルを使用\n",
    "hps = utils.get_hparams_from_file(\"./configs/uudb_ms_istft_vits.json\")\n",
    "\n",
    "net_g = SynthesizerTrn(\n",
    "    len(jpn_symbols), # jpn_symbolsを使用\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model).cuda()\n",
    "_ = net_g.eval()\n",
    "\n",
    "# UUDBモデルのチェックポイントを使用\n",
    "_ = utils.load_checkpoint(\"./logs/uudb_11/G_200000.pth\", net_g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full latent representation and then synthesize only the first half\n",
    "# 日本語のカタカナテキストを入力\n",
    "text = \"コノテキストハゼンタイノオンソノハンブンヲゴウセイスルタメノサンプルデス\"\n",
    "stn_tst = get_text(text, hps)\n",
    "with torch.no_grad():\n",
    "    x_tst = stn_tst.cuda().unsqueeze(0)\n",
    "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).cuda()\n",
    "    \n",
    "    # Perform inference to get the full latent representation (z) and mask (y_mask)\n",
    "    # The infer method returns: o, o_mb, attn, y_mask, (z, z_p, m_p, logs_p)\n",
    "    _, _, _, y_mask, (z, _, _, _) = net_g.infer(x_tst, x_tst_lengths, noise_scale=.667, noise_scale_w=0.8, length_scale=1)\n",
    "\n",
    "    # Determine the length for the first half of the latent representation\n",
    "    # We take half of the sequence length of z\n",
    "    half_len = z.shape[2] // 2\n",
    "    \n",
    "    # Slice the latent representation (z) and its corresponding mask (y_mask)\n",
    "    z_partial = z[:, :, :half_len]\n",
    "    y_mask_partial = y_mask[:, :, :half_len]\n",
    "\n",
    "    # Directly call the decoder (net_g.dec) with the partial latent representation\n",
    "    # The 'g' parameter (speaker embedding) is None in this single-speaker example\n",
    "    audio_partial, _ = net_g.dec((z_partial * y_mask_partial), g=None)\n",
    "    \n",
    "    # Convert the generated audio tensor to a numpy array for playback\n",
    "    "audio_partial = audio_partial[0,0].data.cpu().float().numpy()\n",
    "\n",
    "# Display the synthesized partial audio\n",
    "ipd.display(ipd.Audio(audio_partial, rate=hps.data.sampling_rate, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}